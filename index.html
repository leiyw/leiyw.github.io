<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Yunwen Lei </title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Yunwen Lei </h1>
</div>
<table class="imgtable"><tr><td>
<img src="Lei.jpg" alt="Yunwen Lei" />&nbsp;</td>
<td align="left"><p>PhD,<br /> <a href="https://www.birmingham.ac.uk/schools/computer-science/index.aspx">School of Computer Science</a>, <br /><a href="https://www.birmingham.ac.uk/index.aspx">University of Birmingham</a><br />
Edgbaston<br />
Birmingham
B15 2TT<br />
United Kingdom<br />
E-mail: <i>y.lei</i> [@] bham.ac.uk</p>
</td></tr></table>
<h2>About Me</h2>
<p>I am a Lecturer at School of Computer Science, University of Birmingham. Previously, I was a <a href="http://www.humboldt-foundation.org/pls/web/pub_hn_query.humboldtianer_details?p_lang=en&amp;p_externe_id=4268809">Humboldt Research Fellow</a> at University of Kaiserslautern, a Research Assistant Professor at Southern University of Science and Technology, and a Postdoctoral Research Fellow at City University of Hong Kong. I obtained my PhD degree in Computer Science at Wuhan University in 2014. I am also an External Affiliated Faculty in ML@UA Lab <a href="https://sites.google.com/view/mlualbany">https://sites.google.com/view/mlualbany</a>.</p>
<h2>Research</h2>
<p>My research interests lie in the areas of machine learning and learning theory, with emphasis on the following topics: online learning, deep learning, optimization and extreme classification. In particular, I am interested in developing and analyzing scalable optimization methods for large-scale learning problems.</p>
<h3>Journal Publications </h3>
<ol>
<li><p>Y. Lei and Y. Ying. "Stochastic Proximal AUC Maximization.". Journal of Machine Learning Research, (to appear) 2021.</p>
</li>
<li><p>Y. Lei, T. Hu and K. Tang. "Generalization Performance of Multi-pass Stochastic Gradient Descent with Convex Loss Functions". Journal of Machine Learning Research, 22(25):1−41, 2021.</p>
</li>
<li><p>Y. Lei, T. Hu, G. Li and K. Tang. "Stochastic Gradient Descent for Nonconvex Learning without Bounded Gradient Assumptions". IEEE Transactions on Neural Networks and Learning Systems, 31(10):4394-4400, 2020.</p>
</li>
<li><p>Y. Lei and D.-X. Zhou. "Convergence of Online Mirror Descent". Applied Computational and Harmonic Analysis, 48(1):343-373, 2020. <a href="slide/ACHA2020.pdf">Talk Slides</a></p>
</li>
<li><p>S.-B. Lin, Y. Lei and D.-X. Zhou. "Boosted Kernel Ridge Regression: Optimal Learning Rates and Early Stopping". Journal of Machine Learning Research, 20(46):1-36, 2019.</p>
</li>
<li><p>Y. Lei, U. Dogan, D.-X. Zhou and M. Kloft. "Data-dependent Generalization Bounds for Multi-class Classification". IEEE Transactions on Information Theory, 65(5): 2995-3021, 2019. <a href="slide/TIT2019.pdf">Talk Slides</a></p>
</li>
<li><p>Y. Lei and D.-X. Zhou. "Analysis of Singular Value Thresholding Algorithm for Matrix Completion". Journal of Fourier Analysis and Applications, 25 (6):2957-2972, 2019.</p>
</li>
<li><p>N. Yousefi, Y. Lei, M. Kloft, M. Mollaghasemi and G. Anagnostopoulos. "Local Rademacher Complexity-based Learning Guarantees for Multi-task Learning". Journal of Machine Learning Research, 19(38):1-47, 2018.</p>
</li>
<li><p>Y. Lei, L. Shi and Z.-C. Guo. "Convergence of Unregularized Online Learning Algorithms". Journal of Machine Learning Research, 18(171):1-33, 2018.</p>
</li>
<li><p>Y. Lei and D.-X. Zhou. "Learning Theory of Randomized Sparse Kaczmarz Method". SIAM Journal on Imaging Sciences, 11(1):547-574, 2018.</p>
</li>
<li><p>J. Lin, Y. Lei, B. Zhang and D.-X. Zhou. "Online Pairwise Learning Algorithms with Convex Loss Functions". Information Sciences, 406-407(9):57-70, 2017.</p>
</li>
<li><p>Y. Lei and D.-X. Zhou. "Analysis of Online Composite Mirror Descent Algorithm". Neural Computation, 29(3):825-860, 2017.</p>
</li>
<li><p>Y. Lei and Y. Ying. "Generalization Analysis of Multi-modal Metric Learning". Analysis and Applications, 14(4): 503-521, 2016.</p>
</li>
<li><p>Y. Lei, L. Ding and W. Zhang. "Generalization Performance of Radial Basis Function Networks". IEEE Transactions on Neural Networks and Learning Systems, 26(3):551-564, 2015.</p>
</li>
</ol>
<h3>Conference Publications </h3>
<ol>
<li><p>Z. Yang, Y. Lei, S. Lyu and Y. Ying. "Stability and Differential Privacy of Stochastic Gradient Descent for Pairwise Learning with Non-Smooth Loss". In International Conference on Artificial Intelligence and Statistics, (to appear) 2021.</p>
</li>
<li><p>Y. Lei and Y. Ying. "Sharper Generalization Bounds for Learning with Gradient-dominated Objective Functions". In International Conference on Learning Representations, (to appear) 2021.</p>
</li>
<li><p>A. Ledent, W. Mustafa, Y. Lei and M. Kloft. "Norm-based Generalisation Bounds for Convolutional Neural Networks". In AAAI Conference on Artificial Intelligence, (to appear) 2021.</p>
</li>
<li><p>L. Wu, A. Ledent, Y. Lei and M. Kloft. "Fine-grained Generalization Analysis of Vector-valued Learning". In AAAI Conference on Artificial Intelligence, (to appear) 2021.</p>
</li>
<li><p>Y. Lei, A. Ledent and M. Kloft. "Sharper Generalization Bounds for Pairwise Learning". In Advances in Neural Information Processing Systems, 2020. <a href="slide/NeurIPS2020.pdf">Talk Slides</a></p>
</li>
<li><p>Z. Yang, B. Zhou, Y. Lei and Y. Ying. "Stochastic Hard Thresholding Algorithms for AUC Maximization". In IEEE International Conference on Data Mining, 2020.</p>
</li>
<li><p>Y. Lei and Y. Ying. "Fine-Grained Analysis of Stability and Generalization for Stochastic Gradient Descent". In International Conference on Machine Learning, pages 5809-5819, 2020. <a href="slide/ICML2020.pdf">Talk Slides</a></p>
</li>
<li><p>S. Liu, K. Tang, Y. Lei and X. Yao. "On Performance Estimation in Automatic Algorithm Configuration". In Association for the Advancement of Artificial Intelligence, pages 1526&ndash;1536, 2020.</p>
</li>
<li><p>Y. Lei, P. Yang, K. Tang and D.-X. Zhou. "Optimal Stochastic and Online Learning with Individual Iterates". In Advances in Neural Information Processing Systems, pages 5416-5426, 2019.  (<b>Spotlight</b>, <a href="slide/NeurIPS2019.pdf">Talk Slides</a>)</p>
</li>
<li><p>Y. Lei and K. Tang. "Stochastic Composite Mirror Descent: Optimal Bounds with High Probabilities". In Advances in Neural Information Processing Systems, pages 1526-1536, 2018.</p>
</li>
<li><p>Y. Lei, S.-B. Lin and K. Tang. "Generalization Bounds for Regularized Pairwise Learning". In International Joint Conference on Artificial Intelligence, pages 2376-2382, 2018.</p>
</li>
<li><p>Y. Lei, A. Binder, U. Dogan and M. Kloft. "Localized Multiple Kernel Learning-A Convex Approach". In Asian Conference on Machine Learning, 63:81-96, 2016.</p>
</li>
<li><p>Y. Lei, U. Dogan, A. Binder and M. Kloft. "Multi-class SVMs: From Tighter Data-Dependent Generalization Bounds to Novel Algorithms". In Advances in Neural Information Processing Systems, pages 2026-2034, 2015.</p>
</li>
</ol>
<h2>Teaching</h2>
<ul>
<li><p>Artificial Intelligence, University of Birmingham, Spring 2021.</p>
</li>
<li><p>Neural Computation, University of Birmingham, Autumn 2020.</p>
</li>
<li><p>Machine Learning 3: Mathematics of ML, TU Kaiserslautern, Winter 2019/2020.</p>
</li>
<li><p>Intelligent Data Analysis (Lab course), SUSTech, Spring 2019.</p>
</li>
<li><p>Artificial Intelligence (Lab course), SUSTech, Autumn 2018.</p>
</li>
</ul>
<h2>Referee Experience</h2>
<dl>
<dt>Journal</dt>
<dd><p>
<a href="https://www.worldscientific.com/worldscinet/aa">AA</a>, 
<a href="https://www.journals.elsevier.com/applied-and-computational-harmonic-analysis">ACHA</a>, 
<a href="https://www.journals.elsevier.com/journal-of-approximation-theory">JAT</a>, 
<a href="https://www.journals.elsevier.com/journal-of-complexity">JoC</a>, 
<a href="https://www.comsoc.org/publications/journals/ieee-jsac/cfp/machine-learning-communications-and-networks">JSAC</a>,
<a href="https://www.jmlr.org/">JMLR</a>, 
<a href="https://onlinelibrary.wiley.com/journal/19321872">SAM</a>,
<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=18">TIT</a>, 
<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a>,
<a href="https://signalprocessingsociety.org/publications-resources/ieee-transactions-signal-processing/about-ieee-transactions-signal-processing">TSP</a>, 
<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385">TNNLS</a>, 
<a href="https://www.springer.com/journal/10994">MLJ</a>, 
<a href="https://www.journals.elsevier.com/neurocomputing">NEUCOM</a>,
<a href="https://www.journals.elsevier.com/neural-networks">NEUNET</a>,
<a href="https://www.mitpressjournals.org/loi/neco">NEURCOMP</a></p></dd>
</dl>
<dl>
<dt>Conference</dt>
<dd><p>AAAI (2019-2021), ACML (2019, 2020), AISTATS (2016&ndash;2020), COLT (2018), ICLR (2018&ndash;2021), ICML (2018&ndash;2021), IJCAI (2019&ndash;2021), NeurIPS (2016&ndash;2020)</p></dd>
</dl>
</div>
</body>
</html>
