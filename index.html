<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Yunwen Lei </title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Yunwen Lei </h1>
</div>
<table class="imgtable"><tr><td>
<img src="Lei.jpg" alt="Yunwen Lei" />&nbsp;</td>
<td align="left"><p>PhD,<br /> <a href="https://www.birmingham.ac.uk/schools/computer-science/index.aspx">School of Computer Science</a>, <br /><a href="https://www.birmingham.ac.uk/index.aspx">University of Birmingham</a><br />
Edgbaston<br />
Birmingham
B15 2TT<br />
United Kingdom<br />
E-mail: <i>y.lei</i> [@] bham.ac.uk<br /></p>
</td></tr></table>
<h2>About Me</h2>
<p>I am a <a href="https://www.birmingham.ac.uk/staff/profiles/computer-science/academic-staff/lei-yunwen.aspx">Lecturer</a> at School of Computer Science, University of Birmingham. Previously, I was a <a href="https://www.humboldt-foundation.de/en/connect/explore-the-humboldt-network/singleview?tx_rsmavhsolr_solrview%5BpPersonId%5D=1203562&amp;cHash=3ceafd72df0adfba08344809df66d4b8">Humboldt Research Fellow</a> at University of Kaiserslautern, a Research Assistant Professor at Southern University of Science and Technology, and a Postdoctoral Research Fellow at City University of Hong Kong. I obtained my PhD degree in Computer Science from Wuhan University, and my Bachelor degree in Mathematics from Hunan University. </p>
<h2>Research</h2>
<p>My research interests lie in the areas of machine learning and learning theory, with emphasis on the following topics: online learning, deep learning, optimization and extreme classification. In particular, I am interested in developing and analyzing scalable optimization methods for large-scale learning problems.</p>
<h3>Selected Journal Publications </h3>
<ol>
<li><p>P. Wang, Y. Lei, Y. Ying and H. Zhang. "<a href="paper/ACHA2021.pdf">Differentially Private SGD with Non-smooth Losses</a>". Applied Computational and Harmonic Analysis, 56:306-336 2022.</p>
</li>
<li><p>Y. Lei and K. Tang. "<a href="paper/TPAMI2021.pdf">Learning Rates for Stochastic Gradient Descent with Nonconvex Objectives</a>". IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(12): 4505&ndash;4511, 2021.</p>
</li>
<li><p>Y. Lei and Y. Ying. "<a href="paper/19-418.pdf">Stochastic Proximal AUC Maximization</a>". Journal of Machine Learning Research, 22(61):1-45, 2021. </p>
</li>
<li><p>Y. Lei, T. Hu and K. Tang. "<a href="paper/19-716.pdf">Generalization Performance of Multi-pass Stochastic Gradient Descent with Convex Loss Functions</a>". Journal of Machine Learning Research, 22(25):1−41, 2021.</p>
</li>
<li><p>Y. Lei, T. Hu, G. Li and K. Tang. "<a href="paper/TNNLS2020.pdf">Stochastic Gradient Descent for Nonconvex Learning without Bounded Gradient Assumptions</a>". IEEE Transactions on Neural Networks and Learning Systems, 31(10):4394-4400, 2020.</p>
</li>
<li><p>Y. Lei and D.-X. Zhou. "<a href="paper/ACHA2020.pdf">Convergence of Online Mirror Descent</a>". Applied Computational and Harmonic Analysis, 48(1):343-373, 2020. <a href="slide/ACHA2020.pdf">Talk Slides</a></p>
</li>
<li><p>S.-B. Lin, Y. Lei and D.-X. Zhou. "<a href="paper/18-063.pdf">Boosted Kernel Ridge Regression: Optimal Learning Rates and Early Stopping</a>". Journal of Machine Learning Research, 20(46):1-36, 2019.</p>
</li>
<li><p>Y. Lei, U. Dogan, D.-X. Zhou and M. Kloft. "<a href="paper/TIT2019.pdf">Data-dependent Generalization Bounds for Multi-class Classification</a>". IEEE Transactions on Information Theory, 65(5): 2995-3021, 2019. <a href="slide/TIT2019.pdf">Talk Slides</a></p>
</li>
<li><p>Y. Lei and D.-X. Zhou. "<a href="paper/JFAA2019.pdf">Analysis of Singular Value Thresholding Algorithm for Matrix Completion</a>". Journal of Fourier Analysis and Applications, 25 (6):2957-2972, 2019.</p>
</li>
<li><p>N. Yousefi, Y. Lei, M. Kloft, M. Mollaghasemi and G. Anagnostopoulos. "<a href="paper/17-144.pdf">Local Rademacher Complexity-based Learning Guarantees for Multi-task Learning</a>". Journal of Machine Learning Research, 19(38):1-47, 2018.</p>
</li>
<li><p>Y. Lei, L. Shi and Z.-C. Guo. "<a href="paper/17-457.pdf">Convergence of Unregularized Online Learning Algorithms</a>". Journal of Machine Learning Research, 18(171):1-33, 2018.</p>
</li>
<li><p>Y. Lei and D.-X. Zhou. "<a href="paper/SIAM2018.pdf">Learning Theory of Randomized Sparse Kaczmarz Method</a>". SIAM Journal on Imaging Sciences, 11(1):547-574, 2018.</p>
</li>
<li><p>J. Lin, Y. Lei, B. Zhang and D.-X. Zhou. "<a href="paper/Pairwise2016.pdf">Online Pairwise Learning Algorithms with Convex Loss Functions</a>". Information Sciences, 406-407(9):57-70, 2017.</p>
</li>
<li><p>Y. Lei and D.-X. Zhou. "<a href="paper/Neco2016.pdf">Analysis of Online Composite Mirror Descent Algorithm</a>". Neural Computation, 29(3):825-860, 2017.</p>
</li>
<li><p>Y. Lei and Y. Ying. "<a href="paper/AA2016.pdf">Generalization Analysis of Multi-modal Metric Learning</a>". Analysis and Applications, 14(4): 503-521, 2016.</p>
</li>
<li><p>Y. Lei, L. Ding and W. Zhang. "<a href="paper/RBF2015.pdf">Generalization Performance of Radial Basis Function Networks</a>". IEEE Transactions on Neural Networks and Learning Systems, 26(3):551-564, 2015.</p>
</li>
</ol>
<h3>Selected Conference Publications </h3>
<ol>
<li><p>Y. Lei, M. Liu, Y. Ying. "<a href="paper/NeurIPS2021a.pdf">Generalization Guarantee of SGD for Pairwise Learning</a>". In Advances in Neural Information Processing Systems, pages 21216-21228, 2021.  <a href="slide/NeurIPS2021a.pdf">Talk Slides</a></p>
</li>
<li><p>Z. Yang, Y. Lei, P. Wang, T. Yang and Y. Ying. "<a href="paper/NeurIPS2021b.pdf">Simple Stochastic and Online Gradient Descent Algorithms for Pairwise Learning</a>". In Advances in Neural Information Processing Systems, pages 20160-20171, 2021.</p>
</li>
<li><p>A. Ledent, R. Alves, Y. Lei and M. Kloft. "<a href="paper/NeurIPS2021c.pdf">Fine-grained Generalization Analysis of Inductive Matrix Completion</a>". In Advances in Neural Information Processing Systems, pages 25540&ndash;25552, 2021. </p>
</li>
<li><p>Y. Lei, Z. Yang, T. Yang and Y. Ying. "<a href="paper/ICML2021.pdf">Stability and Generalization of Stochastic Gradient Methods for Minimax Problems</a>". In International Conference on Machine Learning, pages 6175-6186, 2021. (<tt>Long Presentation</tt> acceptance rate: 3%) <a href="slide/ICML2021.pdf">Talk Slides</a></p>
</li>
<li><p>Y. Lei and Y. Ying. "<a href="paper/ICLR2021.pdf">Sharper Generalization Bounds for Learning with Gradient-dominated Objective Functions</a>". In International Conference on Learning Representations, 2021. <a href="slide/ICLR2021.pdf">Talk Slides</a></p>
</li>
<li><p>Z. Yang, Y. Lei, S. Lyu and Y. Ying. "<a href="paper/yang21c.pdf">Stability and Differential Privacy of Stochastic Gradient Descent for Pairwise Learning with Non-Smooth Loss</a>". In International Conference on Artificial Intelligence and Statistics, pages 2026-2034, 2021.</p>
</li>
<li><p>Y. Lei, A. Ledent and M. Kloft. "<a href="paper/NeurIPS2020.pdf">Sharper Generalization Bounds for Pairwise Learning</a>". In Advances in Neural Information Processing Systems, pages 21236-21246, 2020. <a href="slide/NeurIPS2020.pdf">Talk Slides</a></p>
</li>
<li><p>Y. Lei and Y. Ying. "<a href="paper/ICML2020.pdf">Fine-Grained Analysis of Stability and Generalization for Stochastic Gradient Descent</a>". In International Conference on Machine Learning, pages 5809-5819, 2020. <a href="slide/ICML2020.pdf">Talk Slides</a></p>
</li>
<li><p>Y. Lei, P. Yang, K. Tang and D.-X. Zhou. "<a href="paper/NeurIPS2019.pdf">Optimal Stochastic and Online Learning with Individual Iterates</a>". In Advances in Neural Information Processing Systems, pages 5416-5426, 2019.  (<tt>Spotlight</tt> acceptance rate: 3%) <a href="slide/NeurIPS2019.pdf">Talk Slides</a></p>
</li>
<li><p>Y. Lei and K. Tang. "<a href="paper/NeurIPS2018.pdf">Stochastic Composite Mirror Descent: Optimal Bounds with High Probabilities</a>". In Advances in Neural Information Processing Systems, pages 1526-1536, 2018.</p>
</li>
<li><p>Y. Lei, S.-B. Lin and K. Tang. "<a href="paper/IJCAI2018.pdf">Generalization Bounds for Regularized Pairwise Learning</a>". In International Joint Conference on Artificial Intelligence, pages 2376-2382, 2018.</p>
</li>
<li><p>Y. Lei, A. Binder, U. Dogan and M. Kloft. "<a href="paper/ACML2016.pdf">Localized Multiple Kernel Learning-A Convex Approach</a>". In Asian Conference on Machine Learning, 63:81-96, 2016.</p>
</li>
<li><p>Y. Lei, U. Dogan, A. Binder and M. Kloft. "<a href="paper/NIPS2015.pdf">Multi-class SVMs: From Tighter Data-Dependent Generalization Bounds to Novel Algorithms</a>". In Advances in Neural Information Processing Systems, pages 2026-2034, 2015.</p>
</li>
</ol>
<h2>Teaching</h2>
<ul>
<li><p>Current Topics in Artificial Intelligence and Machine Learning, University of Birmingham, Spring 2021 (with Jonathan Rowe and Peter Tino).</p>
</li>
<li><p>Artificial Intelligence, University of Birmingham, Spring 2021 (with Shan He, Miqing Li and Kashif Rajpoot).</p>
</li>
<li><p>Neural Computation, University of Birmingham, Autumn 2020 (with Per Kristian Lehre, Jinming Duan and Shan He).</p>
</li>
<li><p>Machine Learning 3: Mathematics of ML, TU Kaiserslautern, Winter 2019/2020 (with Marius Kloft).</p>
</li>
<li><p>Intelligent Data Analysis (Lab course), SUSTech, Spring 2019.</p>
</li>
<li><p>Artificial Intelligence (Lab course), SUSTech, Autumn 2018.</p>
</li>
</ul>
<h2>Referee Experience</h2>
<dl>
<dt>Journal</dt>
<dd><p>
<a href="https://www.worldscientific.com/worldscinet/aa">AA</a>, 
<a href="https://www.journals.elsevier.com/applied-and-computational-harmonic-analysis">ACHA</a>, 
<a href="https://www.journals.elsevier.com/artificial-intelligence">AIJ</a>, 
<a href="https://www.journals.elsevier.com/big-data-research">BDR</a>, 
<a href="https://www.aimspress.com/journal/era">ERA</a>,
<a href="https://www.sciencedirect.com/journal/information-sciences">INS</a>,
<a href="https://www.journals.elsevier.com/journal-of-approximation-theory">JAT</a>, 
<a href="https://www.journals.elsevier.com/journal-of-computational-mathematics-and-data-science">JCMDS</a>,
<a href="https://www.journals.elsevier.com/journal-of-complexity">JoC</a>, 
<a href="https://www.comsoc.org/publications/journals/ieee-jsac/cfp/machine-learning-communications-and-networks">JSAC</a>,
<a href="https://www.jmlr.org/">JMLR</a>, 
<a href="https://onlinelibrary.wiley.com/journal/19321872">SAM</a>,
<a href="https://www.siam.org/publications/journals/siam-journal-on-matrix-analysis-and-applications-simax">SIMAX</a>,
<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=18">TIT</a>, 
<a href="https://dl.acm.org/journal/tkdd">TKDD</a>,
<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=69">TKDE</a>,
<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a>,
<a href="https://signalprocessingsociety.org/publications-resources/ieee-transactions-signal-processing/about-ieee-transactions-signal-processing">TSP</a>, 
<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385">TNNLS</a>, 
<a href="https://www.aimsciences.org/journal/A0000-0001">MFC</a>, 
<a href="https://www.springer.com/journal/10994">MLJ</a>, 
<a href="https://www.journals.elsevier.com/neurocomputing">NEUCOM</a>,
<a href="https://www.journals.elsevier.com/neural-networks">NEUNET</a>,
<a href="https://www.mitpressjournals.org/loi/neco">NEURCOMP</a></p></dd>
</dl>
<dl>
<dt>Conference</dt>
<dd><p>AAAI (2019-2022), ACML (2019&ndash;2021), AISTATS (2016&ndash;2022), COLT (2018), ECML (2021, 2022), ICLR (2018&ndash;2022), ICML (2018&ndash;2022), IJCAI (2019&ndash;2022), NeurIPS (2016&ndash;2021)</p></dd>
</dl>
</div>
</body>
</html>
