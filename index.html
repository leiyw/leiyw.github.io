<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Yunwen Lei </title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Yunwen Lei </h1>
</div>
<table class="imgtable"><tr><td>
<img src="yunwen.jpg" alt="Yunwen Lei" />&nbsp;</td>
<td align="left"><p>PhD,<br /> <a href="https://hkumath.hku.hk/web/index.php">Department of Mathematics</a>, <br /> <a href="https://www.hku.hk/">The University of Hong Kong</a><br />
Pokfulam<br />
Hong Kong<br />
E-mail: <i>leiyw</i> [@] hku.hk<br /></p>
</td></tr></table>
<h2>About Me</h2>
<p>I am an Assistant Professor at the Department of Mathematics, The University of Hong Kong. My research interests lie in the areas of machine learning and learning theory, with emphasis on the following topics: algorithmic stability, deep learning and stochastic optimization. In particular, I am interested in developing and analyzing scalable optimization methods for large-scale learning problems. I obtained my PhD degree in Computer Science from Wuhan University, and my Bachelor degree in Mathematics from Hunan University. </p>
<p><tt>Open Positions</tt>: I am looking for self-motivated PhD students with strong mathematical ablities and (or) programming skills. If you are interested in working on learning theory and optimization with me, please feel free to send me your CV and your detailed transcripts.</p>
<h2>Research</h2>
<h3>Journal Publications </h3>
<ol>
<li><p>T. Hu, X. Liu, K. Ji and Y. Lei. "<a href="https://ieeexplore.ieee.org/abstract/document/10931786">Convergence of Adaptive Stochastic Mirror Descent</a>". IEEE Transactions on Neural Networks and Learning Systems, 2025.</p>
</li>
<li><p>P. Wang, Y. Lei, D. Wang, Y. Ying, D.-X. Zhou. "<a href="paper/Neco25.pdf">Generalization Guarantees of Gradient Descent for Shallow Neural Networks</a>". Neural Computation, 37(2), 344-402, 2025.</p>
</li>
<li><p>J. Fan and Y. Lei. "<a href="paper/ACHA2024.pdf">High-Probability Generalization Bounds for Pointwise Uniformly Stable Algorithm</a>". Applied and Computational Harmonic Analysis, 70 (101632), 2024.</p>
</li>
<li><p>Z. Huang, Y. Lei and A. Kaban. "<a href="paper/Neco2023.pdf">Optimisation and Learning with Randomly Compressed Gradient Updates</a>". Neural Computation, 35 (7): 1234–1287, 2023.</p>
</li>
<li><p>P. Wang, Y. Lei, Y. Ying and H. Zhang. "<a href="paper/ACHA2021.pdf">Differentially Private SGD with Non-smooth Losses</a>". Applied Computational and Harmonic Analysis, 56:306-336, 2022.</p>
</li>
<li><p>Y. Lei and K. Tang. "<a href="paper/TPAMI2021.pdf">Learning Rates for Stochastic Gradient Descent with Nonconvex Objectives</a>". IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(12): 4505&ndash;4511, 2021.</p>
</li>
<li><p>Y. Lei and Y. Ying. "<a href="paper/19-418.pdf">Stochastic Proximal AUC Maximization</a>". Journal of Machine Learning Research, 22(61):1-45, 2021. </p>
</li>
<li><p>Y. Lei, T. Hu and K. Tang. "<a href="paper/19-716.pdf">Generalization Performance of Multi-pass Stochastic Gradient Descent with Convex Loss Functions</a>". Journal of Machine Learning Research, 22(25):1−41, 2021.</p>
</li>
<li><p>Y. Lei, T. Hu, G. Li and K. Tang. "<a href="paper/TNNLS2020.pdf">Stochastic Gradient Descent for Nonconvex Learning without Bounded Gradient Assumptions</a>". IEEE Transactions on Neural Networks and Learning Systems, 31(10):4394-4400, 2020.</p>
</li>
<li><p>Y. Lei and D.-X. Zhou. "<a href="paper/ACHA2020.pdf">Convergence of Online Mirror Descent</a>". Applied Computational and Harmonic Analysis, 48(1):343-373, 2020. <a href="slide/ACHA2020.pdf">Talk Slides</a></p>
</li>
<li><p>S.-B. Lin, Y. Lei and D.-X. Zhou. "<a href="paper/18-063.pdf">Boosted Kernel Ridge Regression: Optimal Learning Rates and Early Stopping</a>". Journal of Machine Learning Research, 20(46):1-36, 2019.</p>
</li>
<li><p>Y. Lei, U. Dogan, D.-X. Zhou and M. Kloft. "<a href="paper/TIT2019.pdf">Data-dependent Generalization Bounds for Multi-class Classification</a>". IEEE Transactions on Information Theory, 65(5): 2995-3021, 2019. <a href="slide/TIT2019.pdf">Talk Slides</a></p>
</li>
<li><p>Y. Lei and D.-X. Zhou. "<a href="paper/JFAA2019.pdf">Analysis of Singular Value Thresholding Algorithm for Matrix Completion</a>". Journal of Fourier Analysis and Applications, 25 (6):2957-2972, 2019.</p>
</li>
<li><p>N. Yousefi, Y. Lei, M. Kloft, M. Mollaghasemi and G. Anagnostopoulos. "<a href="paper/17-144.pdf">Local Rademacher Complexity-based Learning Guarantees for Multi-task Learning</a>". Journal of Machine Learning Research, 19(38):1-47, 2018.</p>
</li>
<li><p>Y. Lei, L. Shi and Z.-C. Guo. "<a href="paper/17-457.pdf">Convergence of Unregularized Online Learning Algorithms</a>". Journal of Machine Learning Research, 18(171):1-33, 2018.</p>
</li>
<li><p>Y. Lei and D.-X. Zhou. "<a href="paper/SIAM2018.pdf">Learning Theory of Randomized Sparse Kaczmarz Method</a>". SIAM Journal on Imaging Sciences, 11(1):547-574, 2018.</p>
</li>
<li><p>J. Lin, Y. Lei, B. Zhang and D.-X. Zhou. "<a href="paper/Pairwise2016.pdf">Online Pairwise Learning Algorithms with Convex Loss Functions</a>". Information Sciences, 406-407(9):57-70, 2017.</p>
</li>
<li><p>Y. Lei and D.-X. Zhou. "<a href="paper/Neco2016.pdf">Analysis of Online Composite Mirror Descent Algorithm</a>". Neural Computation, 29(3):825-860, 2017.</p>
</li>
<li><p>Y. Lei and Y. Ying. "<a href="paper/AA2016.pdf">Generalization Analysis of Multi-modal Metric Learning</a>". Analysis and Applications, 14(4): 503-521, 2016.</p>
</li>
<li><p>Y. Lei, L. Ding and W. Zhang. "<a href="paper/RBF2015.pdf">Generalization Performance of Radial Basis Function Networks</a>". IEEE Transactions on Neural Networks and Learning Systems, 26(3):551-564, 2015.</p>
</li>
</ol>
<h3>Conference Publications </h3>
<ol>
<li><p>S. Zeng and Y. Lei. "<a href="https://openreview.net/forum?id=g4eTrS2U8o">Stability and Generalization Analysis of Decentralized SGD: Sharper Bounds Beyond Lipschitzness and Smoothness</a>". In International Conference on Machine Learning, 2025.</p>
</li>
<li><p>B. Wang, Y. Lei, Y. Ying and T. Yang. "<a href="https://openreview.net/forum?id=s15HrqCqbr">On Discriminative Probabilistic Modeling for Self-Supervised Representation Learning</a>". In International Conference on Learning Representations, 2025.</p>
</li>
<li><p>S. Zhou, Y. Lei and A. Kaban. "<a href="paper/NeurIPS2023.pdf">Toward Better PAC-Bayes Bounds for Uniformly Stable Algorithms</a>". Advances in Neural Information Processing Systems, 36, pages 29602&ndash;29614, 2023.</p>
</li>
<li><p>Y. Lei.  "<a href="paper/COLT2023.pdf">Stability and Generalization of Stochastic Optimization with Nonconvex and Nonsmooth Problems</a>". In Conference on Learning Theory, pages 191-227, 2023.</p>
</li>
<li><p>Y. Lei, T. Yang, Y. Ying and D.-X. Zhou.  "<a href="paper/ICML2023.pdf">Generalization Analysis for Contrastive Representation Learning</a>". In  International Conference on Machine Learning, pages 19200-19227, 2023.</p>
</li>
<li><p>S. Fu, Y. Lei, Q. Cao, X. Tian and D. Tao. "<a href="https://openreview.net/forum?id=8E5Yazboyh">Sharper Bounds for Uniformly Stable Algorithms with Stationary mixing Process</a>". In International Conference on Learning Representations, 2023.</p>
</li>
<li><p>Y. Lei, R. Jin and Y. Ying. "<a href="paper/NeurIPS2022a.pdf">Stability and Generalization Analysis of Gradient Methods for Shallow Neural Networks</a>". In Advances in Neural Information Processing Systems, pages 38557&ndash;38570, 2022.</p>
</li>
<li><p>P. Wang, Y. Lei, Y. Ying and D.-X. Zhou. "<a href="paper/NeurIPS2022b.pdf">Stability and Generalization for Markov Chain Stochastic Gradient Methods</a>". In Advances in Neural Information Processing Systems, pages 37735&ndash;37748, 2022.</p>
</li>
<li><p>M. Liu, Z. Zhang, Y. Lei and C. Liao. "<a href="paper/NeurIPS2022c.pdf">A Communication-Efficient Distributed Gradient Clipping Algorithm for Training Deep Neural Networks</a>". In Advances in Neural Information Processing Systems, pages 26204&ndash;26217, 2022. (<tt>Spotlight</tt> acceptance rate: 5%)</p>
</li>
<li><p>W. Mustafa, Y. Lei and M. Kloft. "<a href="paper/ICML2022.pdf">On the Generalization Analysis of Adversarial Learning</a>". In International Conference on Machine Learning, pages 16174&ndash;16196, 2022.</p>
</li>
<li><p>Z. Yang, S. Hu, Y. Lei, K. Varshney, S. Lyu, Y. Ying. "<a href="paper/UAI2022.pdf">Differentially Private SGDA for Minimax Problems</a>". In Uncertainty in Artificial Intelligence, pages 2192-2202, 2022.</p>
</li>
<li><p>Y. Lei, M. Liu and Y. Ying. "<a href="paper/NeurIPS2021a.pdf">Generalization Guarantee of SGD for Pairwise Learning</a>". In Advances in Neural Information Processing Systems, pages 21216-21228, 2021.  <a href="slide/NeurIPS2021a.pdf">Talk Slides</a></p>
</li>
<li><p>Z. Yang, Y. Lei, P. Wang, T. Yang and Y. Ying. "<a href="paper/NeurIPS2021b.pdf">Simple Stochastic and Online Gradient Descent Algorithms for Pairwise Learning</a>". In Advances in Neural Information Processing Systems, pages 20160-20171, 2021.</p>
</li>
<li><p>A. Ledent, R. Alves, Y. Lei and M. Kloft. "<a href="paper/NeurIPS2021c.pdf">Fine-grained Generalization Analysis of Inductive Matrix Completion</a>". In Advances in Neural Information Processing Systems, pages 25540&ndash;25552, 2021. </p>
</li>
<li><p>Y. Lei, Z. Yang, T. Yang and Y. Ying. "<a href="paper/ICML2021.pdf">Stability and Generalization of Stochastic Gradient Methods for Minimax Problems</a>". In International Conference on Machine Learning, pages 6175-6186, 2021. (<tt>Long Presentation</tt> acceptance rate: 3%) <a href="slide/ICML2021.pdf">Talk Slides</a></p>
</li>
<li><p>Y. Lei and Y. Ying. "<a href="paper/ICLR2021.pdf">Sharper Generalization Bounds for Learning with Gradient-dominated Objective Functions</a>". In International Conference on Learning Representations, 2021. <a href="slide/ICLR2021.pdf">Talk Slides</a></p>
</li>
<li><p>Z. Yang, Y. Lei, S. Lyu and Y. Ying. "<a href="paper/yang21c.pdf">Stability and Differential Privacy of Stochastic Gradient Descent for Pairwise Learning with Non-Smooth Loss</a>". In International Conference on Artificial Intelligence and Statistics, pages 2026-2034, 2021.</p>
</li>
<li><p>Y. Lei, A. Ledent and M. Kloft. "<a href="paper/NeurIPS2020.pdf">Sharper Generalization Bounds for Pairwise Learning</a>". In Advances in Neural Information Processing Systems, pages 21236-21246, 2020. <a href="slide/NeurIPS2020.pdf">Talk Slides</a></p>
</li>
<li><p>Y. Lei and Y. Ying. "<a href="paper/ICML2020.pdf">Fine-Grained Analysis of Stability and Generalization for Stochastic Gradient Descent</a>". In International Conference on Machine Learning, pages 5809-5819, 2020. <a href="slide/ICML2020.pdf">Talk Slides</a></p>
</li>
<li><p>Y. Lei, P. Yang, K. Tang and D.-X. Zhou. "<a href="paper/NeurIPS2019.pdf">Optimal Stochastic and Online Learning with Individual Iterates</a>". In Advances in Neural Information Processing Systems, pages 5416-5426, 2019.  (<tt>Spotlight</tt> acceptance rate: 3%) <a href="slide/NeurIPS2019.pdf">Talk Slides</a></p>
</li>
<li><p>Y. Lei and K. Tang. "<a href="paper/NeurIPS2018.pdf">Stochastic Composite Mirror Descent: Optimal Bounds with High Probabilities</a>". In Advances in Neural Information Processing Systems, pages 1526-1536, 2018.</p>
</li>
<li><p>Y. Lei, S.-B. Lin and K. Tang. "<a href="paper/IJCAI2018.pdf">Generalization Bounds for Regularized Pairwise Learning</a>". In International Joint Conference on Artificial Intelligence, pages 2376-2382, 2018.</p>
</li>
<li><p>Y. Lei, A. Binder, U. Dogan and M. Kloft. "<a href="paper/ACML2016.pdf">Localized Multiple Kernel Learning-A Convex Approach</a>". In Asian Conference on Machine Learning, 63:81-96, 2016.</p>
</li>
<li><p>Y. Lei, U. Dogan, A. Binder and M. Kloft. "<a href="paper/NIPS2015.pdf">Multi-class SVMs: From Tighter Data-Dependent Generalization Bounds to Novel Algorithms</a>". In Advances in Neural Information Processing Systems, pages 2026-2034, 2015.</p>
</li>
</ol>
<h2>Academic Service</h2>
<dl>
<dt>Top reviewer at NeurIPS (2024, 2019), AISTATS (2022) and IJCAI (2023). TMLR Expert Reviewer (2023).</dt>
<dd><p></p></dd>
<dt>Action Editor</dt>
<dd><p></p></dd>
</dl>
<ul>
<li><p><a href="https://link.springer.com/journal/10994">Machine Learning</a></p>
</li>
<li><p><a href="https://www.aimsciences.org/mfc">Mathematical Foundations of Computing</a></p>
</li>
<li><p><a href="https://www.jmlr.org/tmlr/">Transactions on Machine Learning Research</a></p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385">IEEE Transactions on Neural Networks and Learning Systems</a></p>
</li>
</ul>
<dl>
<dt>Area Chair</dt>
<dd><p></p></dd>
</dl>
<ul>
<li><p>NeurIPS 2025, DSAA 2025, ICLR 2026, AISTATS 2026</p>
</li>
</ul>
<dl>
<dt>Journal Reviewer</dt>
<dd><p></p></dd>
</dl>
<ul>
<li><p><a href="https://www.worldscientific.com/worldscinet/aa">AA</a>, 
<a href="https://www.journals.elsevier.com/applied-and-computational-harmonic-analysis">ACHA</a>, 
<a href="https://www.journals.elsevier.com/artificial-intelligence">AIJ</a>, 
<a href="https://www.sciencedirect.com/journal/information-sciences">INS</a>,
<a href="https://www.journals.elsevier.com/journal-of-approximation-theory">JAT</a>, 
<a href="https://www.journals.elsevier.com/journal-of-computational-mathematics-and-data-science">JCMDS</a>,
<a href="https://www.journals.elsevier.com/journal-of-complexity">JoC</a>, 
<a href="https://www.comsoc.org/publications/journals/ieee-jsac/cfp/machine-learning-communications-and-networks">JSAC</a>,
<a href="https://www.jmlr.org/">JMLR</a>, 
<a href="https://www.pnas.org/">PNAS</a>,
<a href="https://www.siam.org/publications/journals/siam-journal-on-matrix-analysis-and-applications-simax">SIMAX</a>,
<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=18">TIT</a>, 
<a href="https://dl.acm.org/journal/tkdd">TKDD</a>,
<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=69">TKDE</a>,
<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a>,
<a href="https://signalprocessingsociety.org/publications-resources/ieee-transactions-signal-processing/about-ieee-transactions-signal-processing">TSP</a>, 
<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385">TNNLS</a>,  
<a href="https://www.springer.com/journal/10994">MLJ</a>, 
<a href="https://www.journals.elsevier.com/neurocomputing">NEUCOM</a>,
<a href="https://www.journals.elsevier.com/neural-networks">NEUNET</a>,
<a href="https://www.mitpressjournals.org/loi/neco">NEURCOMP</a></p>
</li>
</ul>
<dl>
<dt>Conference Reviewer</dt>
<dd><p></p></dd>
</dl>
<ul>
<li><p>AAAI (19&ndash;22, 25, 26), ACML (19&ndash;25), AISTATS (16&ndash;25), ALT (24&ndash;26), ECML (21&ndash;23), ECAI (24, 25), ICLR (18&ndash;25), ICML (18&ndash;25), IJCAI (19&ndash;25), NeurIPS (16&ndash;24), UAI (23-25) </p>
</li>
</ul>
</div>
</body>
</html>
