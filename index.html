<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Yunwen Lei </title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Yunwen Lei </h1>
</div>
<table class="imgtable"><tr><td>
<img src="Lei.jpg" alt="Yunwen Lei" />&nbsp;</td>
<td align="left"><p>PhD,<br /> <a href="https://www.informatik.uni-kl.de/en/">Department of Computer Science</a>, <br /><a href="https://www.uni-kl.de/en/">University of Kaiserslautern</a><br />
Building 36, Room 308<br />
P.O. Box 3049<br />
67653 Kaiserslautern<br />
E-mail: <i>yunwen.lei</i> [@] hotmail [DOT] com</p>
</td></tr></table>
<h2>About me</h2>
<p>I am currently an Alexander von Humboldt Research Fellow at the Machine Learning group of University of Kaiserslautern, headed by <a href="https://ml.informatik.uni-kl.de/">Prof. Marius Kloft</a>. Previously, I was a Research Assistant Professor at Southern University of Science and Technology, and a Postdoctoral Research Fellow at City University of Hong Kong. I obtained my PhD degree in Computer Science at Wuhan University in 2014.</p>
<h2>Research</h2>
<p>My research interests include </p>
<ul>
<li><p>Learning Theory </p>
</li>
<li><p>Optimization</p>
</li>
<li><p>Machine Learning</p>
</li>
</ul>
<h3>Selected Publications </h3>
<ol>
<li><p>Y. Lei, Y. Ying. "Fine-Grained Analysis of Stability and Generalization for Stochastic Gradient Descent". International Conference on Machine Learning, 2020. <a href="slide/ICML2020.pdf">Talk Slides</a></p>
</li>
<li><p>Y. Lei, T. Hu, G. Li and K. Tang. "Stochastic Gradient Descent for Nonconvex Learning without Bounded Gradient Assumptions". IEEE Transactions on Neural Networks and Learning Systems, <a href="https://doi.org/10.1109/TNNLS.2019.2952219">doi: 10.1109/TNNLS.2019.2952219</a></p>
</li>
<li><p>Y. Lei and D.-X. Zhou. "Convergence of Online Mirror Descent". Applied Computational and Harmonic Analysis, 48(1):343-373, 2020. <a href="slide/ACHA2020.pdf">Talk Slides</a></p>
</li>
<li><p>Y. Lei, P. Yang, K. Tang and D.-X. Zhou. "Optimal Stochastic and Online Learning with Individual Iterates". In Advance in Neural Information Processing Systems, pages 5416-5426, 2019.  (<b>Spotlight</b>, <a href="slide/NeurIPS2019.pdf">Talk Slides</a>)</p>
</li>
<li><p>S.-B. Lin, Y. Lei and D.-X. Zhou. "Boosted Kernel Ridge Regression: Optimal Learning Rates and Early Stopping". Journal of Machine Learning Research, 20(46):1-36, 2019.</p>
</li>
<li><p>Y. Lei, U. Dogan, D.-X. Zhou and M. Kloft. "Data-dependent Generalization Bounds for Multi-class Classification". IEEE Transactions on Information Theory, 65(5): 2995-3021, 2019. <a href="slide/TIT2019.pdf">Talk Slides</a></p>
</li>
<li><p>Y. Lei and K. Tang. "Stochastic Composite Mirror Descent: Optimal Bounds with High Probabilities". In Advance in Neural Information Processing Systems, pages 1526-1536, 2018.</p>
</li>
<li><p>N. Yousefi, Y. Lei, M. Kloft, M. Mollaghasemi and G. Anagnostopoulos. "Local Rademacher Complexity-based Learning Guarantees for Multi-task Learning". Journal of Machine Learning Research, 19(38):1-47, 2018.</p>
</li>
<li><p>Y. Lei, L. Shi and Z.-C. Guo. "Convergence of Unregularized Online Learning Algorithms". Journal of Machine Learning Research, 18(171):1-33, 2018.</p>
</li>
<li><p>Y. Lei and D.-X. Zhou. "Learning Theory of Randomized Sparse Kaczmarz Method". SIAM Journal on Imaging Sciences, 11(1):547-574, 2018.</p>
</li>
<li><p>Y. Lei, U. Dogan, A. Binder and M. Kloft. "Multi-class SVMs: From Tighter Data-Dependent Generalization Bounds to Novel Algorithms". In Advances in Neural Information Processing Systems, pages 2026-2034, 2015.</p>
</li>
</ol>
<h2>Teaching</h2>
<ul>
<li><p>Machine Learning 3: Mathematics of ML (Lecture course), TU Kaiserslautern, Winter 2019/2020.</p>
</li>
<li><p>Intelligent Data Analysis (Lab course), SUSTech, Spring 2019.</p>
</li>
<li><p>Artificial Intelligence (Lab course), SUSTech, Autumn 2018.</p>
</li>
</ul>
<h2>Referee Experience</h2>
<dl>
<dt>Journal</dt>
<dd><p>
<a href="https://www.jmlr.org/">JMLR</a>, 
<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=18">TIT</a>, 
<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a>,
<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385">TNNLS</a>, 
<a href="https://www.journals.elsevier.com/applied-and-computational-harmonic-analysis">ACHA</a>, 
<a href="https://www.worldscientific.com/worldscinet/aa">AA</a>, 
<a href="https://www.journals.elsevier.com/journal-of-complexity">JoC</a>, 
<a href="https://www.journals.elsevier.com/journal-of-approximation-theory">JAT</a>, 
<a href="https://www.comsoc.org/publications/journals/ieee-jsac/cfp/machine-learning-communications-and-networks">JSAC</a>,
<a href="https://www.springer.com/journal/10994">Machine Learning</a>, 
<a href="https://www.journals.elsevier.com/neural-networks">Neural Networks</a>, 
<a href="https://www.journals.elsevier.com/neurocomputing">Neurocomputing</a></p></dd>
</dl>
<dl>
<dt>Conference</dt>
<dd><p>AAAI (2019, 2020), ACML (2019, 2020), AISTATS (2016&ndash;2020), COLT (2018), ICLR (2018&ndash;2020), ICML (2018&ndash;2020), IJCAI (2019, 2020), NeurIPS (2016&ndash;2020)</p></dd>
</dl>
</div>
</body>
</html>
